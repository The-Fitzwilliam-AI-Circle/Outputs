# Abandon normal instruments

- What if you couldn't issue a call for proposals?
- The grant application is a filter - what does it filter out?
- Stop measuring. Go visit.
- What did Bell Labs do instead of program design?

# Accept advice

- Ask the person who left research why they left
- What do the rejected applicants know about your program?
- The historian of science has seen this before
- What would the researchers ask for if "fundable" weren't a constraint?

# Accretion

- Fund the community before the project
- Let them meet three times before you ask what they're doing
- Small bets reveal what big bets should be
-  The program can grow into its theory of change

# A line has two sides

- Legibility to funders is illegibility to inventors
- Selection is also rejection - what does your filter destroy?
- The accountability that protects you constrains them
- A program that can't fail can't discover

# Allow an easement

- Which reporting requirement exists for audit, not insight?
- What if the timeline weren't annual?
- The milestone assumes you know where you're going
- Let them change the question

# Are there sections? Consider transitions

- Where does scoping end and commissioning begin? You've been scoping for a year
- The gap between program design and program delivery is where the theory dies
- The transition from fundamental research to translation — you keep funding one or the other, never the crossing
- What happens between the workshop and the follow-up? That's where the community either forms or dissipates

# Ask people to work against their better judgment

- Ask the experimentalist to theorize. Ask the theorist to build something
- Fund the researcher who thinks their idea won't work yet
- Commission the review from someone outside the field. Their "naive" framing is the fresh one
- Ask the senior scientist to submit the idea they'd never put in a grant application

# Ask your body

- Which program makes you nervous? That's probably the one that might actually change something
- You keep returning to a topic in conversation that isn't in your portfolio. Follow it
- The community you find most exciting to visit — what is it about them?
- Your gut says this researcher will deliver. Your process says they're too risky. Which signal do you trust?

# Assemble some of the instruments in a group and treat the group

- Your five separate programs share a problem. What if you connected them?
- Fund the collaboration, not the individual labs. Make the group the grantee
- The biologist, the engineer, and the ethicist don't need separate funding streams. They need a shared room
- Combine the scoping study, the community workshop, and the pilot fund into one program. The separation is artificial

# Balance the consistency principle with the inconsistency principle

- Consistent enough that researchers trust the funding landscape. Inconsistent enough that you surprise them
- Have a strategy. Break it occasionally. The strategic surprise reveals what the strategy was missing
- Some programs need ten-year stability. Some need to be torn up after eighteen months. Know which is which
- Consistent criteria, inconsistent topics. Or consistent topics, inconsistent criteria. Pick one

# Be dirty

- Fund the messy proposal with the exciting idea over the clean proposal with the safe one
- The rough draft of the program is more honest than the polished strategy deck
- Let the workshop be chaotic. Structure kills the conversations you need most
- Ship the funding call before it's perfect. The community's response will tell you what's missing

# Breathe more deeply

- You're funding in twelve-month cycles. The problem operates on a twenty-year timescale
- How much of the research landscape have you actually seen? Travel further
- Double the size of the bet you're most confident about
- What would this program look like if you had ten times the patience?

# Bridges -build -burn

- Connect the two communities that don't know each other. The bridge is the program
- Burn the bridge between "basic" and "applied" — it was always a fiction that served administrators
- Build a relationship between the funder and the funded that isn't mediated by a proposal
- Connect the policy need to the research capacity. Neither side knows the other exists

# Cascades

- One well-placed person moves to a new institution. What did that do to the field?
- A single workshop can create a community that outlasts the program that funded it
- Fund the training, which produces the people, who create the labs, which train the next generation. Where do you intervene?
- One program's output becomes another program's input. Design for that

# Change instrument roles

- The evaluation isn't just a check — it's a research tool. What is the evaluation teaching you about how research works?
- The program manager isn't an administrator. They're a curator. Fund that role accordingly
- The annual report shouldn't just report. It should be a publication
- The rejection letter is a steering mechanism. What are you steering the field toward with your rejections?

# Change nothing and continue with immaculate consistency

- The program has been running for five years and the field trusts it. Don't redesign it because a new director arrived
- Keep the same review criteria. The comparability across cohorts is the value
- Consistency of support is itself the intervention. Researchers plan around reliable funding
- Don't pivot to the trendy topic. The unglamorous program you've sustained for a decade is producing the deep work

# Children's voices -speaking -singing

- How would you explain this program to a member of the public? If you can't, the theory of change is unclear
- What would a new PhD student think is missing from the funding landscape? They haven't learned to not see the gaps
- The undergraduate asks "why isn't anyone working on this?" — that's a scoping question
- The child asks "why?" four times. By the fourth time, you're at the real justification for the program

# Cluster analysis

- Your portfolio has natural groupings that don't match your organizational chart. What are they?
- Which researchers keep appearing in different programs? They're at the intersection. That intersection might be a field
- Cluster the unfunded proposals. What does the demand look like? The demand reveals the gaps
- Which problems keep appearing across disciplines? The repeated emergence is the signal

# Consider different fading systems

- How should a program end? Abruptly? With a taper? With a transition to another funder?
- The community you built will outlast your funding. How do you plan for that?
- Some programs should fade into the background as the field matures. Others should end with a bang — a definitive answer
- Your attention to a topic fades naturally. What does the portfolio look like when you're not actively managing it?

# Consult other sources -promising -unpromising

- Read the DARPA case studies. Read the Wellcome Trust retrospectives. What did they learn about program design?
- The unpromising source: the venture capitalist, the intelligence analyst, the film producer. They all curate portfolios of uncertain bets
- What does the sociology of science say about how breakthroughs actually happen? It's not what funders assume
- Talk to the retired program officer. They've seen which interventions actually worked, twenty years later

# Convert a melodic element into a rhythmic element

- Stop asking what to fund. Ask when to fund it. Timing is the intervention
- The content of the research matters less than the cadence of interaction between the researchers
- Convert your theory of change from a narrative into a sequence of events. What's the rhythm of the program?
- The pattern of funding decisions — bursts, steady drips, long silences — is itself a signal to the community

# Courage!

- Fund the person everyone thinks is wrong. If they're right, the field shifts
- Cancel the program that isn't working. Don't let it coast to the end of its funding period
- Name the problem nobody wants to name. The neglected area is neglected for political reasons, not intellectual ones
- Commission the evaluation that might show your program didn't work

# Cut a vital connection

- What happens if you remove the intermediary between funder and researcher? Fund them directly
- Disconnect the program from the policy justification for one cycle. What does it become?
- What if the program officer couldn't attend the review panel? Does the selection change?
- Cut the link between reporting and funding continuation. What do they report when it's no longer performative?

# Decorate, decorate

- Add a history of science component to every program. What did they learn about how fields form?
- Collect ethnographic data alongside the research outputs. How people work is as interesting as what they produce
- Commission art about the research. The artist sees something the impact report doesn't capture
- Record the workshop conversations, not just the conclusions. The process of arriving at the idea is the meta-research

# Define an area as 'safe' and use it as an anchor

- One core program you believe in absolutely. Everything experimental is measured against it
- Establish what you know works — community-building, long-term fellowships — before you experiment with new formats
- The trusted review process is your anchor. Innovate around it, not through it
- Keep one relationship with the research community that is purely about listening, with no funding attached

# Destroy -nothing -the most important thing

- Close your most successful program. Force the portfolio to evolve
- Destroy nothing: keep the failed programs in the institutional memory. They're case studies
- What happens to the field if the funding disappears entirely? The answer reveals what was self-sustaining and what was artificial
- Delete the strategy document. Start the next one from the researchers, not from the mission statement

# Discard an axiom

- What if peer review doesn't work for selecting the most impactful research?
- What if the principal investigator model is the problem, not the solution?
- Drop the assumption that research needs a question before it needs funding
- What if the best research programs can't be designed in advance?

# Disconnect from desire

- You want this program to produce a breakthrough. That desire is distorting your evaluation criteria
- You want the community to be grateful. That desire is distorting the power dynamic
- Stop wanting the researchers to stay on topic. Their drift might be the discovery
- You're attached to the elegance of the program design. The design is for the work, not for you

# Discover the recipes you are using and abandon them

- You always commission a scoping study, then a workshop, then a call. What if you started with the call?
- You always fund PIs at universities. What about the independent researcher, the industry scientist, the retired expert?
- Your "open call" has a template that determines the shape of the answers. Change the template
- You always evaluate at the end. What if you evaluated at the beginning and designed the program from what you learned?

# Distorting time

- Fund the six-month sprint and the twenty-year endowment in the same portfolio. Both are needed
- What if the deadline weren't a deadline but a window? Rolling applications change who applies
- Compress the review cycle. Three months is too long. The idea has mutated by the time you respond
- What would you fund if you were designing a program in 1960? In 2060? The difference reveals your assumptions about the present

# Do nothing for as long as possible

- Don't launch the program yet. Spend six months listening to the field before you design anything
- Resist the urge to intervene in the funded project. Let them struggle. The struggle is the research
- The best scoping study is doing nothing while the community self-organizes around the problem
- Don't fill the gap in the portfolio. Maybe the gap is right. Maybe nobody should be working on that

# Don't be afraid of things because they're easy to do

- A mailing list connecting twelve people is cheap and might catalyze a field
- Travel grants are unglamorous. They create more collaborations than any structured program
- Hosting a dinner is easy. It's also how half the important scientific relationships start
- Funding a postdoc is the simplest intervention. It's also how new research directions begin

# Don't be frightened of cliches

- The investigator-initiated grant. It's a cliche because researchers know what to work on
- The sabbatical visit. Old-fashioned and effective
- Funding excellent people and leaving them alone. Everyone says it. It works
- The annual retreat where funded researchers present to each other. Simple, standard, valuable

# Don't be frightened to display your talents

- You can see that these two fields should be talking to each other. Make the introduction. That's curatorial skill
- Your judgment about which researcher is undervalued is a talent. Act on it
- You understand the policy landscape and the research landscape. The translation is your contribution
- You've read more proposals than anyone. Your pattern recognition across fields is a unique instrument

# Don't break the silence

- Not every gap in the funding landscape needs to be filled. Some silences are appropriate
- The researchers haven't asked for help with this. Maybe they don't need it
- The absence of a program in an area is itself a position. Make it a deliberate one
- Don't fill the post-workshop silence with another workshop. Let them think

# Don't stress one thing more than another

- You're overweighting the flagship program and neglecting the seed fund. The seed fund is where the next flagship comes from
- Impact metrics and process metrics deserve equal attention
- The small grant and the large grant both need proper design. Don't phone in the small one
- You focus on what you fund. What about what you convene, what you connect, what you decline?

# Do something boring

- Read all the final reports from the last five years. Every one. What's the aggregate picture?
- Map the funding landscape properly. Who else funds in this area? How much? For how long?
- Audit your review process. Are the criteria actually applied consistently?
- Update the database of contacts, communities, and capabilities. The infrastructure of relationships is boring and essential

# Do the washing up

- Close out the finished programs properly. Final reports, lessons learned, thank-you letters
- Clean up the portfolio tracker. Half the entries are out of date
- Respond to the inquiries that have been sitting in the inbox for three weeks
- Archive the old program documents properly. Someone will need them when they design the next version

# Do the words need changing?

- "Impact" means something different to the funder, the researcher, and the policymaker. Align the definitions or stop using the word
- You call it a "program." The researchers experience it as a community, a constraint, and a lifeline. Which framing is true?
- "Translational research" — translating what, to whom, through what mechanism?
- Is it "de-risking" or "exploring"? The language changes what you look for and what you tolerate

# Do we need holes?

- Does every year need a funding round? The gap year might produce better proposals
- Does every project need a milestone review? The un-reviewed project might take the risk you want
- The hole in your portfolio — the area you don't fund — might be attracting funding from others precisely because you left space
- Not every researcher needs to be in the network. The outsider's independence is productive

# Emphasize differences

- Two programs with similar goals produced different outcomes. The difference is the design insight
- Compare your portfolio to another funder's. The divergence is your niche — or your blind spot
- The community that formed organically and the one you convened. What's different about the work they produce?
- The same researcher funded through two different mechanisms. Which version of their work is better?

# Emphasize repetitions

- The same gap keeps appearing in every landscape review. Stop reviewing. Start funding
- Three independent communities converged on the same question. That convergence is the signal
- The same type of project keeps failing. The failure is systematic, not individual
- Researchers keep requesting the same thing you don't offer. The request is the design specification

# Emphasize the flaws

- The program that failed taught you more about research design than the one that succeeded
- Your most controversial funding decision — unpack why it was controversial. The controversy is about values, not evidence
- The review process rejected the proposal that later won the Nobel Prize. Study the rejection, not the prize
- Your portfolio's weakest area is the one most worth examining. Why is it weak?

# Faced with a choice, do both

- Top-down program design or bottom-up responsive mode? Run both and compare
- Fund the individual or fund the team? Fund both structures for the same question
- Short-term deliverables or long-term exploration? Split the portfolio
- Should the program be open or targeted? Make the first round open, the second targeted, using what you learned

# Feedback recordings into an acoustic situation

- Show the researchers what the portfolio looks like from above. They see their project. You see the pattern
- Feed the evaluation findings back into the program design. Close the loop
- Let the first cohort's experience redesign the call for the second cohort
- Publish what you learned from the program. Let the field respond. The response is the next program's input

# Fill every beat with something

- Every interaction with a researcher is a data point about the field. Capture it
- The site visit isn't just oversight — it's scouting, relationship-building, and learning. Make every visit count
- Between funding rounds, the program should still be active: convening, connecting, scanning
- Every rejection letter is an opportunity to steer, to encourage, to redirect. Don't waste it

# Get your neck massaged

- You've been in strategy meetings for a month. Go visit a lab
- Switch from designing programs to reading the science. Remember what you're designing for
- Talk to someone who does the research, not someone who manages it
- You've been thinking about research. Go think about something else for a week. The connection will come

# Ghost echoes

- The program ended five years ago. The community it created is still publishing together. That's the real output
- The funder that used to operate in this space left a gap. The gap still shapes the landscape
- The researcher you didn't fund went on to do the work anyway, differently. Your rejection shaped their approach
- The old program's framing still constrains how people think about the question, even though the funding stopped

# Give the game away

- Publish your strategy document before you finalize it. Let the community shape it
- Share your decision-making criteria openly. The field deserves to know the rules
- Tell other funders what you're planning so they can coordinate, not compete
- Make the program's theory of change public. If it's wrong, the community will tell you faster than any evaluation

# Give way to your worst impulse

- Fund the project you can't justify to the board. Use the discretionary budget
- Commission the work that is too early, too weird, too small for any official program
- Follow the hunch. The instinct that violates the process is sometimes the curatorial talent
- Fund the angry researcher. The anger is about something real

# Go slowly all the way round the outside

- Before you design the program, map the entire ecosystem: who funds what, who works where, who talks to whom
- Walk the perimeter of the problem before you define the center. What are the adjacent fields?
- Visit every lab in the area before you write the call. Your understanding of the landscape is the program design
- Survey the whole community before you select the theme. The theme should emerge, not be imposed

# Honor thy error as a hidden intention

- The program attracted the wrong community. But the community that showed up might be the right one for a different question
- The funded project went off-topic. The tangent produced the finding the program needed
- You misjudged the readiness of the field. The premature program created the readiness
- The evaluation showed no impact. But the process of evaluating changed how the team thinks about impact

# How would you have done it?

- How did the Rockefeller Foundation design programs in the 1930s? What did they know that we've forgotten?
- How would you fund this research if you had no bureaucracy — just a chequebook and a phone?
- If you were the researcher, what would you want the funder to do? And what would you want them to stop doing?
- How would a venture studio approach this problem? Resident teams, shared infrastructure, rapid iteration

# Humanize something free of error

- The program metrics are perfect. Every milestone met. But did anything surprising happen? If not, the bar was too low
- The process was flawless. But the researchers feel managed, not supported. The process serves the funder, not the work
- The portfolio is balanced. But balance might mean you're not making strong enough bets
- The review was fair. But fairness selected for the legible over the imaginative

# Imagine the music as a moving chain or caterpillar

- The research program is a process, not a structure. Watch it evolve over time
- The ideas move from person to person, lab to lab, country to country. Fund the movement, not the location
- The pipeline — discovery to application — is a chain. Which link is weakest? Fund that link
- The field is a living thing, developing. Your intervention is at one point in its lifecycle. Know which point

# Imagine the music as a set of disconnected events

- What if your funded projects aren't a portfolio but a collection of independent bets?
- Each workshop is a standalone event. Stop assuming continuity between them
- The individual researcher's insight doesn't need to connect to the program narrative. Let it stand alone
- What if the program's outputs are useful independently, even if the program's thesis is wrong?

# Infinitesimal gradations

- The difference between a funded and an unfunded proposal might be one paragraph. Is that paragraph real, or is it noise?
- Small changes in the call language produce large changes in who applies. Test the sensitivity
- The difference between a thriving community and a dormant one is often one person. Find that person
- Adjust the program by small increments between cohorts. Each adjustment is an experiment

# Intentions -credibility of -nobility of -humility of

- Are you designing this program because the problem is important or because the program is fundable?
- Your stated intention is to catalyze a field. Your actual intervention is to fund twelve labs. Is that the same thing?
- Humility: the field might solve this problem without you. Your role might be to accelerate, not to initiate
- The intention to create impact is noble. But impact defined by whom, measured when, compared to what counterfactual?

# Into the impossible

- Fund the research area where no methodology currently exists. The methodology will follow the funding
- Try to create a new field. Not a program — a field. What would that require?
- Attempt to coordinate funders across five countries around a single question. The coordination problem is the real challenge
- Commission a ten-year forecast of a scientific field. It will be wrong, but the exercise reveals assumptions

# Is it finished?

- The program achieved its goals three years ago. You've been running it out of institutional inertia
- The community can sustain itself now. Your funding is no longer catalytic — it's habitual
- The landscape has changed. The program was designed for a world that no longer exists
- The question has been answered. Or it's been superseded. Either way, the program is finished. Close it with dignity

# Is there something missing?

- Your portfolio has no capacity-building. You're funding projects but not building the people who do them
- There's no mechanism for the unfundable idea — the one that doesn't fit any call. Create one
- You fund research but not the infrastructure that makes research possible. Who funds the databases, the shared tools, the training?
- The community has no way to talk to you outside the application process. The missing channel is the one with the most information

# Is the tuning appropriate?

- Your grant size is too large for exploration and too small for implementation. You're in the dead zone
- The review panel has too many people to have a real conversation and too few for statistical reliability
- Your program timeline is set by the financial year, not by the research question. Is the mismatch acceptable?
- The level of reporting you require — is it calibrated to the size of the grant and the stage of the work?

# Just carry on

- The program hasn't produced visible results yet. It's been two years. Continue. Fields take a decade
- The first cohort didn't gel. The second might. Keep convening
- The strategy feels uncertain. That's because you're working on hard problems. Stay the course
- Other funders are moving into your space. That's validation, not a reason to leave

# Left channel, right channel, center channel

- The researcher's view, the funder's view, the public's view. Three perspectives on the same program
- Basic research, applied research, policy engagement. Three streams that need different management
- Academic output, industrial uptake, public understanding. Which channel is your program optimized for? What about the other two?
- The funded, the unfunded, the unaware. Three populations that experience your program completely differently

# Listen in total darkness, or in a very large room, very quietly

- Attend the conference without presenting. Just listen to what researchers talk about in the corridors
- Read the proposals without knowing who wrote them. What do you fund when reputation is removed?
- Remove all the metrics. What is your impression of the portfolio's health?
- Sit in on a lab meeting. Don't introduce yourself as a funder. Listen to the science without the performance

# Listen to the quiet voice

- The junior researcher at the back of the workshop had an idea. The senior people talked over it. Go back to it
- The application that scored just below the funding line. What was in it?
- The field that nobody champions because it has no natural home in any department
- Your own doubt about the program's direction. The quiet institutional voice that says something isn't right

# Look at a very small object; look at its center

- Take one funded project. Understand everything about how the funding changed the work. That's your evaluation
- One researcher's career trajectory. What did each funding intervention do? That's your theory of change, tested
- One workshop interaction that led to a collaboration. Trace it. How did the program create the conditions?
- The single paragraph in the call for proposals that determined who applied. Examine it

# Look at the order in which you do things

- You design, then consult, then fund. What if you funded first, then designed the program from what emerged?
- You select the topic, then the people. What if you selected the people first and let them define the topic?
- You scope, then commission. What if scoping and commissioning were the same activity — paid exploratory work?
- The evaluation happens at the end. What if it happened at the beginning, as a baseline that makes the end meaningful?

# Look closely at the most embarrassing details and amplify them

- The program's most embarrassing output is the one worth studying most carefully. What does it reveal about your selection?
- The funding decision you're least proud of. Was it the process that failed, or the judgment?
- The community you built fell apart after the funding ended. That's the most important finding about sustainability
- Your theory of change has a step that says "and then innovation happens." Expand that step. That's where the work is

# Lowest common denominator check -single beat -single note -single riff

- Can you describe the program's purpose in one sentence? If not, the design is unclear
- Would a single unrestricted grant to the right person achieve more than the entire structured program?
- The simplest intervention: put two people in a room who should be talking. Start there
- What's the minimum viable program? One researcher, one question, one year. What can that produce?

# Make a blank valuable by putting it in an exquisite frame

- The program produced no breakthrough. But the rigorous documentation of why the field isn't ready is the contribution
- The landscape review that finds nothing is only valuable if the methodology is sound enough to trust the absence
- No fundable proposals in this area. With a well-designed call, that's evidence about the state of the field, not a failure of outreach
- The null result from the program evaluation: no measurable impact. With proper counterfactual analysis, that's a finding about how research funding works

# Make an exhaustive list of everything you might do and do the last thing on the list

- The last thing on the list: fund the person, not the project. No deliverables. No milestones. Just trust
- You'd never fund the artist to work alongside the scientists. That's at the bottom. Try it
- The intervention you'd never propose to the board — embedding a funder inside a lab for a year — might teach you more than any evaluation
- Commission the research you know least about. Your ignorance is the signal that nobody else is funding it either

# Make a sudden, destructive, unpredictable action; incorporate

- Cancel a call mid-process. Redesign it from the feedback of the applicants who were preparing
- Merge two programs that have been running separately into one. See what the collision produces
- Announce a funding opportunity with a one-week deadline. Who responds reveals who's ready
- Invite a critic of your entire approach to join the advisory board. The disruption is the governance working

# Mechanicalize something idiosyncratic

- The program officer's intuition about which researchers will collaborate well. Can you write that as selection criteria?
- You know a good proposal when you see one. Break "good" into its components. What are you actually detecting?
- The informal network of contacts you use for scoping. Map it. Is it representative? Is it biased?
- The way you sense a field is "ready" for a program. What signals are you reading? Can they be measured?

# Mute and continue

- Remove the program officer from the community interactions. Let the researchers self-organize
- Stop communicating the program's goals. See what the researchers produce when they're not performing to the brief
- Withdraw the reporting requirement for one cycle. Does the work change?
- Step back from the advisory board's direction. Let the portfolio drift. Where does it go?

# Only one element of each kind

- One program, one question, one community, one timeline. Resist the urge to add complexity
- Fund one person in the field. Just one. If the field is real, that person will find the others
- One event per year for the community. Make it count
- What's the single intervention that would have the most leverage? Do only that

# (Organic) machinery

- The research community is a self-organizing system. Your funding is a nutrient, not a blueprint
- Let the program grow from the community's needs, not from the strategy document
- The best programs develop their own internal logic. The initial design was scaffolding. Remove it when the structure holds
- The field is an ecosystem. You're adjusting the soil chemistry, not planting individual seeds

# Overtly resist change

- Keep the program running as designed. The researchers need stability more than they need the latest institutional pivot
- Resist the pressure to align with the new government priority. The program's value is its independence
- Don't chase the trend. The unfashionable topic you've been funding for ten years is about to become important
- Keep the same review panel for continuity. Institutional memory in the panel is an asset

# Put in earplugs

- Stop listening to what other funders are doing. Your differentiation depends on your own vision
- Ignore the media coverage of the field. The media covers what's exciting, not what's important
- Don't read the policy documents before you design the program. Design from the science first
- Tune out the advisory board for one cycle. What does your own judgment tell you?

# Remember those quiet evenings

- The first program you designed. The one that was naive and earnest and smaller than anything you'd do now. What did it get right?
- Before the strategy frameworks and the impact metrics — why did you want to fund research?
- The conversation with the researcher who changed your understanding of their field. That conversation is the model for everything
- When was the last time you were genuinely surprised by a funded project? What's preventing that from happening now?

# Remove ambiguities and convert to specifics

- "We fund high-risk, high-reward research." What risk? Whose reward? How high?
- "The program aims to build capacity." How many people, in which skills, to what level, by when?
- "We seek transformative science." Transformative of what? Specify the thing you want changed
- "The portfolio is diverse." Diverse across what axes? Geography, discipline, methodology, seniority? Measure it

# Remove specifics and convert to ambiguities

- Stop specifying deliverables. Specify the problem and let the researchers choose the deliverable
- Don't define success. Let the community define it after the work is done
- Widen the call. The more specific the brief, the fewer surprises. The fewer surprises, the less catalysis
- Remove the topic boundaries. What would the researchers do with fully undirected funds?

# Repetition is a form of change

- The third iteration of the same workshop with the same people produces something the first two couldn't. The repetition built the trust
- You've funded this lab three times. Each time, the relationship is different. The continuity is the intervention
- Running the same program format again isn't stagnation — it's building institutional knowledge about what that format does
- Every review panel you run teaches you something new about the field. The repetition is the learning mechanism

# Reverse

- Instead of asking "what should we fund?" ask "what should we stop funding?"
- Start from the impact you want and work backwards to the research. Does the chain of logic actually hold?
- Don't ask researchers what they need. Ask the people who need the research what they need from researchers
- Reverse the power dynamic: let the researchers interview you. What do they learn about your constraints?

# Short circuit

- Skip the scoping study. You already know enough. Fund the work
- Bypass the review panel. Fund on the program officer's judgment. See what happens
- Skip the workshop. Send the email introduction. The two people will figure it out themselves
- Remove every step in the process that doesn't add information. How many steps survive?

# Shut the door and listen from outside

- What does your program look like from the researcher's perspective? Not what you designed — what they experience
- How would a science journalist describe your portfolio? The outside view strips the institutional framing
- Ask the unfunded researchers what they think your program is for. Their perception is your real communication
- Read your own call for proposals as if you were applying. Would you?

# Simple subtraction

- Remove one requirement from the application form. Which one do you not miss?
- Drop one program from the portfolio. What does the remaining portfolio say about your priorities?
- Eliminate one stage of the review process. Did the quality of selection change?
- Remove the word limit on the proposal. Or halve it. Which produces better proposals?

# Spectrum analysis

- Rank your portfolio from lowest to highest risk. Where's the center of gravity? Is that where you want it?
- Map the distribution of grant sizes. Is the shape intentional or emergent?
- Spread your funded researchers on a spectrum from established to emerging. Who's missing?
- Analyze the distribution of topics across your portfolio. Where are the clusters? Where are the voids?

# Take a break

- The strategy refresh can wait until next quarter. The current strategy is still working
- You've been in back-to-back meetings about research design for a month. Read a novel
- The portfolio doesn't need another intervention. Let it run. Check back in six months
- Step away from the institution for a week. Visit a lab with no agenda. Remember what research feels like from inside

# Take away the elements in order of apparent non-importance

- Remove the annual conference. Remove the newsletter. Remove the mid-term report. At what point does the program lose coherence?
- Strip the program to its core: money to researchers with a shared question. What did all the other infrastructure actually do?
- Remove the selection process — fund everyone who applies. What happens? If nothing bad, the selection was wasted effort
- Take away the program officer's site visits. What information is lost? That's the information the visits were providing

# Tape your mouth

- Stop framing the program narrative. Show the portfolio and let people draw their own conclusions
- Don't tell the review panel what you're looking for. See what they find
- Present the evaluation data without interpretation. The researchers' reaction is the interpretation
- Your theory of change is a story you tell yourself. What does the evidence say without the story?

# The inconsistency principle

- The program works brilliantly in one country and fails in another. The inconsistency is the finding about institutional context
- Two reviewers rate the same proposal as excellent and unfundable. Both are right. The disagreement is about values
- Your portfolio deliberately contradicts itself: some bets assume X is true, others assume X is false. That's proper hedging
- The program's stated goals and actual outputs don't match. Good. The researchers found something better

# The tape is now the music

- The program's real output isn't the papers or the patents. It's the community, the capabilities, the changed researchers
- The process of designing the program — the conversations, the workshops, the debates — was itself the intervention
- The portfolio as it exists today, not as you planned it, is the object. Study what it actually is
- The act of funding shaped the field. The funding wasn't supporting the field — it was creating it

# Think of the radio

- The researcher in the lab doesn't read your strategy document. They experience your program through the call, the review, and the money. Design for what they actually see
- The policymaker needs a two-sentence summary of why this research matters. Can you provide it?
- The public pays for this. What would they understand about where their money goes?
- The next program officer needs to inherit this. Is it designed for continuity or for your personal stewardship?

# Tidy up

- Close out the administrative loose ends of the last three programs before you design the next one
- Reconcile the financial records with the programmatic records. They've diverged
- Update the stakeholder map. People have moved. Institutions have changed. The landscape is not what it was
- Organize the institutional knowledge from the last decade of programs. It's in emails and memories. Put it somewhere durable

# Trust in the you of now

- Your judgment about this field is better informed than it was five years ago. Trust it
- You've seen enough programs to know what works. The intuition is earned
- The team's collective sense that this direction is right is a form of evidence. Act on it
- You don't need another consultation to confirm what you already know. Design the program

# Turn it upside down

- Instead of funding researchers to work on your question, pay them to tell you what the question should be
- What if the program's success criterion were the quality of the failures it produced?
- Instead of selecting the best proposals, randomly fund from the qualified pool. What changes?
- What if the researchers evaluated the funder? Publish the results

# Twist the spine

- You're organized by research theme. Reorganize by methodology. Or by career stage. Or by geography. The twist reveals different gaps
- Shift from funding research to funding the conditions for research: infrastructure, time, community
- Move the program's center of gravity from the university to the hospital, the factory, or the field
- You've been thinking about supply of research. Think about demand. Who needs this knowledge and why isn't it reaching them?

# Use an old idea

- The Haldane principle: researchers, not funders, should set the research agenda. When did you last actually follow it?
- Bell Labs model: hire excellent people, give them a problem space, provide long-term security. Can you approximate that?
- The Rothschild model: the customer commissions the research. Who is the customer for yours?
- Vannevar Bush's "Science: The Endless Frontier." Still the foundational argument. Reread it. What holds up?

# Use an unacceptable color

- Fund the research that nobody would put in a grant application because it sounds absurd
- Support the field that reviewers consistently rank low because it's unfashionable, not because it's unimportant
- Fund the researcher with no institutional affiliation, no track record, and a brilliant idea
- Commission work on the problem everybody acknowledges but nobody will fund because it's politically sensitive

# Use fewer notes

- One call, one criterion: will this change how we understand the problem? Everything else is administration
- Reduce the application to one page. What survives the compression is the idea
- One program, fully resourced, instead of five programs, each underfunded
- The leaner the process, the more of the budget reaches the research. Cut until it hurts

# Use filters

- Only fund researchers who have been rejected by every other funder. Your niche is the unfundable
- Only review proposals from researchers you've met in person. Selection by relationship, not by document
- Filter your portfolio to the projects that surprised you. What do they have in common?
- Only consider the programs that wouldn't exist without you. If someone else would fund it, let them

# Use "unqualified" people

- Put a patient on the review panel for the biomedical program
- Let the early-career researcher design the next call. They know what's missing because they're living it
- Invite the science fiction writer to the scoping workshop. Their job is imagining what doesn't exist yet
- The civil servant who will implement the policy output should be in the room when the research question is defined

# Water

- Let the funding flow to where the ideas are, not to where the institutions are
- The program should be liquid: adaptable, filling the shape of the community it serves
- Money is water. It finds its own level. If you have to force it somewhere, ask whether the gradient is real
- The best programs have no rigid borders. The ideas flow between them, and that's the design working

# What are you really thinking about just now? Incorporate

- You keep circling back to a topic that isn't in any program. That's your next program
- The conversation at lunch was more interesting than the strategy meeting. What was it about?
- You're distracted by a problem in a completely different domain. There's a structural analogy to your portfolio. Find it
- The thing that keeps you up at night isn't on the workplan. Put it there

# What is the reality of the situation?

- Your budget is fixed. You can't fund everything you've identified as important. What are the real priorities?
- The research community is small. You're funding half of it. Your decisions shape the field whether you intend to or not
- The political context constrains what you can fund. Acknowledge the constraint instead of pretending neutrality
- Your influence is real and asymmetric. The researchers need you more than you need them. Design with that power imbalance in mind

# What mistakes did you make last time?

- Last time you designed the program alone and consulted afterwards. The consultation was performative. This time, co-design
- The previous program's milestones were too specific and the researchers gamed them. Loosen the next ones
- You underestimated how long it takes to build a community. Double the timeline
- The evaluation was commissioned too late and measured the wrong things. Design the evaluation with the program, not after it

# What would your closest friend do?

- Your trusted colleague would say: the portfolio has too many small bets. Concentrate
- Your mentor would say: you're overcomplicating the process. The researchers don't need a theory of change. They need money and time
- Your peer at another institution would say: you've been talking about this program for two years. Launch it
- The person who knows you best would say: you're avoiding the hard decision. Which program should you end?

# What wouldn't you do?

- You'd never fund someone to do nothing for a year and just think. But that's what sabbaticals are, and they produce breakthroughs
- You'd never fund a project with no defined question. But the question might be what the project needs to find
- You'd never give the full budget to one researcher and say "do whatever you want." But what if you did?
- You'd never fund the same project twice. But the second attempt, informed by the first failure, might be the one that works

# Work at a different speed

- One-day turnaround on a seed grant. Speed is an intervention: it selects for readiness
- Ten-year commitment to a single researcher. Slowness is an intervention: it selects for depth
- Run the review panel in an afternoon instead of a week. Forced speed reveals instinct
- Let the application window stay open for a year. Self-selected timing changes who applies

# You are an engineer

- The program is a system. Inputs, outputs, feedback loops, failure modes. Design it as one
- The theory of change is a circuit diagram. Where is the energy lost? Where is the signal?
- Build the evaluation into the program design, not bolted on afterwards. Instrumentation is part of the build
- The funding mechanism is a tool. Match the tool to the job. Grants, prizes, contracts, fellowships — each has different properties

# You can only make one dot at a time

- One program at a time. Launch it, learn from it, then design the next one
- One funding decision at a time. Each one is a commitment, not a placeholder
- One conversation at a time with a researcher. The aggregate of those conversations is your understanding of the field
- You can't reshape the landscape overnight. Each intervention is one dot. The trajectory emerges over years

# You don't have to be ashamed of using your own ideas

- Your instinct about what's missing from the landscape is valid. You don't need a commissioned report to confirm it
- The program design you sketched on the back of an envelope might be better than the one produced by the working group
- Your own experience of the funding system is evidence. You've been on both sides
- The idea you've been developing privately, that doesn't come from any stakeholder — bring it to the table

# [blank white card]

- The intervention that no funding agency has tried is the one this situation demands
- There is no template for what you're trying to do. That's the point
- The blank card is the space where program design becomes invention — not funding research as it exists but creating the conditions for research that doesn't exist yet
- You are designing an instrument for discovery. The design itself is a creative act. There are no rules for this part